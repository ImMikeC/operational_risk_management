---
title: "CASO VIKINGS FISH"
author: "Octavio del Sueldo y Jose López Galdón"
date: "5/6/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Librerías

```{r libraries}

  # Datasets
library(CASdatasets)

  # Manipulación de los datos
library(dplyr)
library(tidyverse)
library(janitor)
library(lubridate)
library(magrittr)

  # Plots
library(ggplot2)

  # Calculos estadísticos estadísticos y actuariales
library(MASS)
library(actuar)
library(moments)
library(purrr)
library(fitdistrplus)
library(evmix)
library(OpVaR)
library(QRM)

```

## BASE DE DATOS

Comenzamos cargando los datos...

```{r load data}

  # Cargamos la base de datos
data("danishuni")

  # Visualizamos los datos
head(danishuni)
tail(danishuni)

  # Dimensión de los datos
dim(danishuni)

  # Comprobamos si hay nulos
colSums(is.na(danishuni))

  # Comprobamos si hay valores duplicados en la fecha
sum(duplicated(danishuni$Date))

```

### Data Transformation

Dado que no tenemos todos los días y hay algunos duplicados vamos a hacer una serie de transformaciones:

```{r transform data}

  # Limpiamos los nombres
data <- danishuni %>%
  clean_names()

  # Creamos el dataset con la frecuencia 
data_freq <- data %>% group_by(date) %>% 
  summarise(freq = n()) %>%
  complete(date = seq.Date(min(date), max(date), by="day"))%>%
  replace(is.na(.), 0) %>%
  group_by(date) %>% 
  summarise(freq = sum(freq))

head(data_freq)

  # Creamos el dataset de severidad
data_sev <- data

head(data_sev)

```


## EDA (Exploratory Data Analysis)

Una vez tenemos nuestro dataset modificado, podemos realizar un análisis exploratorio de los datos...

### Resumen estadístico

```{r summary}

  # Resumen de freq
summary(data_freq)

  # Resumen de la severidad
summary(data_sev)
```

En ambos casos a patir del tercer cuartil observamos valores extremos... Si bien es cierto, que se produce de manera más exagerada con las pérdidas. A continuación analizaremos los percentiles de de ambos datasets:

### Análsis de los cuantiles

#### Frecuencia

En el caso de la frecuencia existe el salto en el percentil 99, lo que muestra una cola a lado derecho.

```{r FREQ quantile analysis}

quantile(data_freq$freq, seq(0.75, 1, 0.01))

```

#### Severidad

Encontramos un gran salto a partil del 99, por lo que la distribución es similar a la anterior. A partir del tercer cuartil se empiezan a ver valores altos...

```{r SEV quantile analysis}

quantile(data_sev$loss, seq(0.5, 1, 0.01))

```

### Asimetría y Curtosis

#### Frecuencia

Para la frecuencia tenemos una asimetría positiva (hacia la derecha) y distribución el leptocúrtica o más apuntada de lo normal...

```{r FREQ skew & kurt analysis}

skewness(data_freq$freq)

kurtosis(data_freq$freq)

```

#### Severidad

En el caso de la severidad es mucho más radical que el anterior... Una alta asimetría positiva y elevada leptocrtosis.

```{r SEV skew & kurt analysis}

skewness(data_sev$loss)

kurtosis(data_sev$loss)

```

### Gráficos de la distribución

#### Frecuencia

En este caso, lo normal es no tener eventos de pérdidas en el día o 1... Pero existen valores extremos como el 4 o el 5.

```{r hist Freq}

ggplot(data_freq, aes(freq)) +
  geom_histogram()

hist(data_freq$freq, pch=10, breaks=30, prob=TRUE, main="Frecuencia",
     xlab =" X", ylab = "Densidad")

```

#### Severidad

En cuanto a la severidad, las pérdidas suelen ser bajas (la media está en torno a 1.8) pero encontramos valores extremos al final de la cola derecha...

```{r hist Loss}

hist(data_sev$loss, pch=10, breaks=100, prob=TRUE, main="Pérdidas",
     xlab =" X", ylab = "Densidad")

lines(density(data_sev$loss), col="red")

```

***

## SELECCIÓN DEL MODELO: INFERENCIA PARAMÉTRICA

- Ajuste de distintas distribuciones

- Contraste de bondad del ajuste

### Severidad

Los modelos de distribuciones de probabilidad relacionados con la severidad que vamos a aplicar a la variable pérdidas son:

1. La ley de Pareto
2. La ley gamma
3. Mixtura entre gamma y Pareto
4. Disrtribución de Burr
5. La Ley Log-Normal

Para este caso práctico, estimaremos por MLE (Maximum Likelihood Estimation - Máxima verosimilitud) y por MGE (maximizing goodness-of-fit estimation - Máxima bondad del ajuste), los compararemos y seleccionaremos la mejor distribución para cada método...

#### Máxima verosimilitud (MLE)

En estadística, la estimación por máxima verosimilitud (conocida también como EMV y, en ocasiones, MLE por sus siglas en inglés) es un método habitual para ajustar un modelo y estimar sus parámetros.

#### Ajuste distribuciones

```{r fit MLE}

  # Ajustamos Pareto por MLE
fit_pareto_MLE <- fitdist(data_sev$loss, distr = "pareto", method = "mle")
fit_pareto_MLE

  # Ajustamos Gamma por MLE
fit_gamma_MLE <- fitdist(data_sev$loss, distr = "gamma", method = "mle")
fit_gamma_MLE

  # Ajustamos Gamma-Pareto por MLE
dmixgampar <- function(x, prob, nu, lambda, alpha, theta)
  prob*dgamma(x, nu, lambda) + (1-prob)*dpareto(x, alpha, theta)
pmixgampar <- function(q, prob, nu, lambda, alpha, theta) 
  prob*pgamma(q, nu, lambda) + (1-prob)*ppareto(q, alpha, theta)

fit_mixtura_MLE <- fitdist(data_sev$loss, distr = "mixgampar", method = "mle", 
                           start = list(prob = 1/2, nu = 1, lambda = 1, alpha = 2, theta = 2), lower = 0)
fit_mixtura_MLE

  # Ajustamos la distribución de Burr
fit_burr_MLE <- fitdist(data_sev$loss, distr = "burr", method = "mle", 
                        start = list(shape1 = 2, shape2 = 2, scale = 2), 
                        lower = c(0.1, 1/2, 0))
fit_burr_MLE$estimate

  # Ajustamos la distribución lognormal
fit_lognormal_MLE <- fitdist(data_sev$loss, distr = "lnorm", method = "mle")
fit_lognormal_MLE

```

#### Visualizaciones

Para observar los ajustes podemos graficar los resultados podemos plotear las densidades las anteriores distribuciones y observar cuál de ellas es la que mejor se ajusta:

```{r plot density MLE}

FDD <- cdfcomp(list(fit_pareto_MLE, fit_gamma_MLE, fit_burr_MLE, fit_lognormal_MLE, fit_mixtura_MLE), xlogscale=TRUE,
            xlab = "Datos en escala logaritmica", ylab = "Probabilidad", datapch=".",
            datacol="black", fitcol=2:6, fitlty=2,
            legendtext=c("Pareto","Gamma", "Burr", "Log-Normal", "Par-Gam"),
            main="Ajuste pérdidas", plotstyle = "ggplot")

FDD
```

Claramente la distribución de Burr (azuk oscuro) es la que mejor se ajusta a las datos reales (representados en negro). Además visualizaremos mediante un QQPLOT (En estadística, un gráfico Q-Q es un método gráfico para el diagnóstico de diferencias entre la distribución de probabilidad de una población de la que se ha extraído una muestra aleatoria y una distribución usada para la comparación) para observar el ajuste al final de cola:

```{r Comparativa ditribution plots ppcom & qqcomp}

qqcomp(list(fit_pareto_MLE, fit_gamma_MLE, fit_burr_MLE, fit_lognormal_MLE), xlogscale=TRUE, ylogscale=TRUE, 
       ylab="cuantiles empíricos", xlab="cuantiles teóricos",
       fitcol=2:5, main="QQ-plot sobre pérdidas", addlegend = TRUE,
       legendtext=c("Pareto","Gamma", "Burr", "Log-Normal"), fitpch=1:4)

ppcomp(list(fit_pareto_MLE, fit_gamma_MLE, fit_burr_MLE, fit_lognormal_MLE), xlogscale=TRUE, ylogscale=TRUE, 
       ylab="Probabilidades empíricas", xlab="Probabilidades teóricas",
       fitcol=2:5, main="PP-plot sobre pérdidas", addlegend = TRUE,
       legendtext=c("Pareto","Gamma", "Burr", "Log-Normal"), fitpch=1:4)

```

En este caso, podemos observar como la Burr sigue siendo la que mejor se ajusta... A continuación, ploteamos el histograma de los datos con la densidad de la Burr (ya que es la mejor):

```{r histogram and burr density}

  # Creanis una variable nueva con la información de las pérdidas
x <- data_sev$loss

  # Ploteamos la desnidad de Burr y el histograma de datos reales
hist(x, pch=10, breaks=100, prob=TRUE, main="PERDIDAS",
     xlab =" X", ylab = "Densidad")
curve(dburr(x, fit_burr_MLE$estimate[1], fit_burr_MLE$estimate[2], fit_burr_MLE$estimate[3]),
      col="red", lwd=2, add=T)

```

Como podemos observar, los datos se ajusntan correctamente en la mayoría de los datos. Sin embargo, esta distribución no es tan leptocurtica como los datos reales, es por ello, que no alcanza la altura en los datos más comunes (el cero), esto no es un problema ya que buscamos modelizar los valores extremos.

#### Bondad de ajuste

```{r testing MLE}

gofstat(list(fit_pareto_MLE, fit_gamma_MLE, fit_burr_MLE, fit_lognormal_MLE, fit_mixtura_MLE), 
        chisqbreaks=c(0:4, 9), discrete= FALSE, 
        fitnames=c("Pareto","Gamma", "Burr", "Log-Normal", "Par-Gam"))

```

Tras realizar la bondasd de ajuste afirmamos las hipótesis planteadas en las visualizaciones, la distribución que mejor ajusta las pérdidas por MLE es la distribución de Burr.


#### Máxima Bondad del Ajuste (MGE)

La bondad de ajuste de un modelo estadístico describe lo bien que se ajusta un conjunto de observaciones. Las medidas de bondad en general resumen la discrepancia entre los valores observados y los valores esperados en el modelo de estudio.

#### Ajuste distribuciones

```{r fit MGE}

  # Ajustamos Pareto por MLE
fit_pareto_MGE <- fitdist(data_sev$loss, distr = "pareto", method = "mge", gof="CvM")
fit_pareto_MGE

  # Ajustamos Gamma por MLE
fit_gamma_MGE <- fitdist(data_sev$loss, distr = "gamma", method = "mge", gof="CvM")
fit_gamma_MGE

  # Ajustamos Gamma-Pareto por MLE
fit_mixtura_MGE <- fitdist(data_sev$loss, distr = "mixgampar", method = "mge", 
                           start = list(prob = 1/2, nu = 1, lambda = 1, alpha = 2, theta = 2), lower = 0, gof="CvM")
fit_mixtura_MGE

  # Ajustamos la distribución de Burr
fit_burr_MGE <- fitdist(data_sev$loss, distr = "burr", method = "mge", 
                        start = list(shape1 = 2, shape2 = 2, scale = 2), 
                        lower = c(0.1, 1/2, 0), gof="CvM")
fit_burr_MGE$estimate

  # Ajustamos la distribución lognormal
fit_lognormal_MGE <- fitdist(data_sev$loss, distr = "lnorm", method = "mge", gof="CvM")
fit_lognormal_MGE

```

#### Visualizaciones

Para observar los ajustes podemos graficar los resultados podemos plotear las densidades las anteriores distribuciones y observar cuál de ellas es la que mejor se ajusta:

```{r plot density MGE}

FDD <- cdfcomp(list(fit_pareto_MGE, fit_gamma_MGE, fit_burr_MGE, fit_lognormal_MGE, fit_mixtura_MGE), xlogscale=TRUE,
            ylab = "Probabilidad", datapch=".",
            datacol="black", fitcol=2:6, fitlty=2,
            legendtext=c("Pareto","Gamma", "Burr", "Log-Normal", "Par-Gam"),
            main="Ajuste pérdidas", plotstyle = "ggplot")

FDD

```

Claramente la distribución de Burr (azuk oscuro) es la que mejor se ajusta a las datos reales (representados en negro). Además visualizaremos mediante un QQPLOT (En estadística, un gráfico Q-Q es un método gráfico para el diagnóstico de diferencias entre la distribución de probabilidad de una población de la que se ha extraído una muestra aleatoria y una distribución usada para la comparación) para observar el ajuste al final de cola:

```{r Comparativa ditribution plots ppcom & qqcomp MGE}

qqcomp(list(fit_pareto_MGE, fit_gamma_MGE, fit_burr_MGE, fit_lognormal_MGE), xlogscale=TRUE, ylogscale=TRUE, 
       ylab="cuantiles empíricos", xlab="cuantiles teóricos",
       fitcol=2:5, main="QQ-plot sobre pérdidas", addlegend = TRUE,
       legendtext=c("Pareto","Gamma", "Burr", "Log-Normal"), fitpch=1:4)

ppcomp(list(fit_pareto_MGE, fit_gamma_MGE, fit_burr_MGE, fit_lognormal_MGE), xlogscale=TRUE, ylogscale=TRUE, 
       ylab="Probabilidades empíricas", xlab="Probabilidades teóricas",
       fitcol=2:5, main="PP-plot sobre pérdidas", addlegend = TRUE,
       legendtext=c("Pareto","Gamma", "Burr", "Log-Normal"), fitpch=1:4)

```

En este caso, podemos observar como la Burr sigue siendo la que mejor se ajusta... A continuación, ploteamos el histograma de los datos con la densidad de la Burr (ya que es la mejor):

```{r histogram and burr density MGE}

  # Creanis una variable nueva con la información de las pérdidas
x <- data_sev$loss

  # Ploteamos la desnidad de Burr y el histograma de datos reales
hist(x, pch=10, breaks=100, prob=TRUE, main="PERDIDAS",
     xlab =" X", ylab = "Densidad")
curve(dburr(x, fit_burr_MLE$estimate[1], fit_burr_MLE$estimate[2], fit_burr_MLE$estimate[3]),
      col="red", lwd=2, add=T)

```

Como podemos observar, los datos se ajusntan correctamente en la mayoría de los datos. Sin embargo, esta distribución no es tan leptocurtica como los datos reales, es por ello, que no alcanza la altura en los datos más comunes (el cero), esto no es un problema ya que buscamos modelizar los valores extremos.

#### Bondad de ajuste

```{r testing MGE}

gofstat(list(fit_pareto_MGE, fit_gamma_MGE, fit_burr_MGE, fit_lognormal_MGE, fit_mixtura_MGE), 
        chisqbreaks=c(0:4, 9), discrete= FALSE, 
        fitnames=c("Pareto","Gamma", "Burr", "Log-Normal", "Par-Gam"))

```

En este caso la Burr es mejor, pero los datos empeoran ligeramente respecto a utilizar el método MLE.


### Frecuencia

Los modelos de distribuciones de probabilidad relacionados con la frecuencia que vamos a aplicar a la variable son:

1. La ley de Poisson
2. La ley Binomial Negativa

Para este caso práctico, estimaremos por MLE (Maximum Likelihood Estimation - Máxima verosimilitud) y por MGE (maximizing goodness-of-fit estimation - Máxima bondad del ajuste), los compararemos y seleccionaremos la mejor distribución para cada método...

#### Máxima verosimilitud (MLE)

En estadística, la estimación por máxima verosimilitud (conocida también como EMV y, en ocasiones, MLE por sus siglas en inglés) es un método habitual para ajustar un modelo y estimar sus parámetros.

#### Ajuste distribuciones

```{r fit MLE FREQ}

  # Ajustamos Poisson por MLE
fit_poisson_MLE <- fitdist(data_freq$freq, distr = "pois", method = "mle")
fit_poisson_MLE

  # Ajustamos binomial negativa por MLE
fit_nbinom_MLE <- fitdist(data_freq$freq, distr = "nbinom", method = "mle")
fit_nbinom_MLE

```

#### Visualizaciones

Para observar los ajustes podemos graficar los resultados podemos plotear las densidades las anteriores distribuciones y observar cuál de ellas es la que mejor se ajusta:

```{r plot density MLE FREQ}

FDD <- cdfcomp(list(fit_poisson_MLE, fit_nbinom_MLE), xlogscale=FALSE,
            ylab = "Probabilidad", datapch=".",
            datacol="black", fitcol=2:3, fitlty=2,
            legendtext=c("Poisson","Binomial Negativa"),
            main="Ajuste frecuencia", plotstyle = "ggplot")

FDD

```

En este gráfico estamos visualizando la distribución de poisson y una binomial negativa para ver el ajuste frente a la frecuencia. Parece ser que la distribución que mejor ajusta el la binomial negativa (verde).

#### Bondad de ajuste

```{r testing MLE FREQ}

gofstat(list(fit_poisson_MLE, fit_nbinom_MLE), 
        chisqbreaks=c(0:4, 9), discrete= TRUE, 
        fitnames=c("Poisson","Binomial Negativa"))

```

Cualquiera de las distribuciones ajusta bastante bien...

#### Método de los Momentos (MME)

En este caso no podemos ajustar el MGE porque es para variables continuas, es por ello que utilizaremos el método de los momentos. En estadística, el método de momentos es un método de estimación de los parámetros poblacionales. Se empieza derivando ecuaciones que relacionan los momentos poblacionales a los parámetros de interés. Por lo tanto, la muestra está definida y los momentos de población están estimados de la muestra.

#### Ajuste distribuciones

```{r fit MME FREQ}

  # Ajustamos Poisson por MLE
fit_poisson_MME <- fitdist(data_freq$freq, distr = "pois", method = "mme")
fit_poisson_MME

  # Ajustamos binomial negativa por MLE
fit_nbinom_MME <- fitdist(data_freq$freq, distr = "nbinom", method = "mme")
fit_nbinom_MME

```


#### Visualizaciones

Para observar los ajustes podemos graficar los resultados podemos plotear las densidades las anteriores distribuciones y observar cuál de ellas es la que mejor se ajusta:

```{r plot density MME FREQ}

FDD <- cdfcomp(list(fit_poisson_MME, fit_nbinom_MME), xlogscale=FALSE,
            ylab = "Probabilidad", datapch=".",
            datacol="black", fitcol=2:3, fitlty=2,
            legendtext=c("Poisson","Binomial Negativa"),
            main="Ajuste frecuencia", plotstyle = "ggplot")

FDD

```

En este gráfico estamos visualizando la distribución de poisson y una binomial negativa para ver el ajuste frente a la frecuencia. Parece ser que la distribución que mejor ajusta el la binomial negativa (verde).

#### Bondad de ajuste

```{r testing MME FREQ}

gofstat(list(fit_poisson_MME, fit_nbinom_MME), 
        chisqbreaks=c(0:4, 9), discrete= TRUE, 
        fitnames=c("Poisson","Binomial Negativa"))

```

Cualquiera de las distribuciones ajusta bastante bien...

***

## ANÁLISIS DE VALORES EXTREMOS

La Teoría de Valores Extremos (EVT de sus siglas en inglés) es la rama de la estadística que centra su estudio en los eventos asociados a las colas de la distribución (valores más altos o más bajos de la variable sometida a estudio).

Utilizada para predecir las posibilidades de eventos que nunca anteriormente han ocurrido.

La modelización de las colas de la distribución, puede desarrollarse a través de diferentes estrategias:

- *Block Maxima*: Se basa en el ajuste de la distribución de los valores máximos o mínimos.
- El análisis de los valores extremos se realiza a partir del análisis de los valores que exceden cierto umbral, denominado *“Peaks Over Threshold*

### Block Maxima

En este caso vamos a sacar los máximos por años:

```{r select max from years}

  # Seleccionamos los 4 primeros elementos de la fecha, es decir, el año
years <- as.numeric(substr(data_sev$date, 1, 4))

  # Agrupamos por año y extraemos la máxima pérdida por año
danish.max <- aggregate(data_sev$loss, by=list(years), max, na.rm=TRUE)[,2]

  # Visualizamos el resultado
danish.max

```

### Peaks Over Threshold

Establecemos un valor umbral de 25 ya que es cuando se produce el salto del percentil 99 y 100%

```{r POT}

  # Establecemos el umbral
u <- 25
danish.exc <- data_sev[data_sev[,2] > u, 2]

danish.exc

  # Nº de casos que superan u
n.u <- length(danish.exc) 
n.u

  # Resumen
summary(danish.exc)


# Det. prob empiricas de la muestra
surv.prob <- 1 - (rank(danish.exc)/(n.u + 1))
surv.prob
# El valor 263.25 tiene la prob mas baja de ocurrencia


  # Ploteamos los excesos
plot(danish.exc, surv.prob, log = "xy", xlab = "Excesos", 
     ylab = "Probabilidades", ylim=c(0.01, 1))

#Se observa linealidad y decreciente.La pendiente es - (Cov/varianza), si cov negativa.


#Añadimos las prob. teoricas de la D.Pareto con estimador por minimos cuadrados de alfa

alpha <- - cov(log(danish.exc), log(surv.prob)) / var(log(danish.exc)) # -(cov)/var
alpha 

x = seq(u, max(danish.exc), length = 100) #divide de u a max() 100 interv.
x
y = (x / u)^(-alpha)

lines(x, y)


#Funcion de distribucion acumulada
prob <- rank(danish.exc) / (n.u + 1)
plot(danish.exc, prob, log = "x", xlab= "Excesos", ylab = "Probabilidades de no exceder")

y = 1 - (x / u)^(-alpha)

lines(x, y)

```

### Distribucion de valores extremos generalizados (GEV).

La teoría de valores extremos o análisis de valores extremos es una rama de la estadística que trata de las desviaciones respecto a al valor esperado de una distribución de probabilidad.

```{r GEV}
# mu = posicion
# sigma = escala
# epsilon = cola o inicio de la cola

  # Definimos la funcion GEV
nllik.gev <- function(par, data){
  mu <- par[1]
  sigma <- par[2]
  xi <- par[3]
  if ((sigma <= 0) | (xi <= -1))
    return(1e6)
  n <- length(data)
  if (xi == 0)
    n * log(sigma) + sum((data - mu) / sigma) +
    sum(exp(-(data - mu) / sigma))
  else {
    if (any((1 + xi * (data - mu) / sigma) <= 0))
      return(1e6)
    n * log(sigma) + (1 + 1 / xi) *
      sum(log(1 + xi * (data - mu) / sigma)) +
      sum((1 + xi * (data - mu) / sigma)^(-1/xi))
    }
  }

  # Inicializamos los parametros para ajustar la distribucion
sigma.start <- sqrt(6) * sd(danish.max) / pi
mu.start <- mean(danish.max) + digamma(1) * sigma.start

  # Ajustamos la distribucion GEV a los datos
fit.gev <- nlm(nllik.gev, c(mu.start, sigma.start, 0),
                 hessian = TRUE, data = danish.max)
fit.gev
  #par.posicion, escala y forma
fit.gev$estimate 

sqrt(diag(solve(fit.gev$hessian))) 
```

En este caso podemos observar que mu es igual a 37.79, la escala es 28.93 y el indice de cola o epsilon es 0.63. Por lo tanto, la distribucion se corresponderia con una Gambel o Frechet.

### Modelo Poisson-Generalizada de Pareto

En teoría de la probabilidad y en estadística, la distribución Pareto es una distribución de probabilidad continua con dos parámetros, que tiene aplicación en disciplinas como la sociología, geofísica y economía.

```{r Modelo Poisson-Generalizada de Pareto}
#Modelo Poisson-Generalizada de Pareto 

nllik.gp <- function(par, u, data){
  tau <- par[1]
  xi <- par[2]
    if ((tau <= 0) | (xi < -1))
      return(1e6)
    m <- length(data)
      if (xi == 0)
        m * log(tau) + sum(data - u) / tau
    else {
        if (any((1 + xi * (data - u) / tau) <= 0))
          return(1e6)
      m * log(tau) + (1 + 1 / xi) *
         sum(log(1 + xi * (data - u) / tau))
       }
     }

  # u es el umbral
u <- 25
tau.start <- mean(danish.exc) - u 
fit.gp <- nlm(nllik.gp, c(tau.start, 0), u = u, hessian = TRUE,
              data = danish.exc)
fit.gp 

  # parametros
fit.gp$estimate 
sqrt(diag(solve(fit.gp$hessian)))
#El parametro eta m/n=0,005
```

Los parametros son escala y forma y obtenemos por resultados 10.40 y 0.82 respectivamente. Esto significa que la cola de la distribucion es pesada. Esto es porque el indice de cola (forma) es mayor a cero

```{r QQ Plot DGP}
#### VALIDACION DEL MODELO

#Q-Q Plot para la Dist. Generalizada de Pareto (DGP)

qqgpd <- function(data, u, tau, xi){
  excess <- data[data > u]
  m <- length(excess)
  prob <- 1:m / (m + 1)
  x.hat <- u + tau / xi * ((1 - prob)^-xi - 1)
    ylim <- xlim <- range(x.hat, excess)
    plot(sort(excess), x.hat, xlab = "Quantiles en la muestra",
           ylab = "Quantiles ajustados", xlim = xlim, ylim = ylim)
    abline(0, 1, col = "grey")
    }

u <- 25
tau <- fit.gp$estimate[1]
indice_cola <- fit.gp$estimate[2]
qqgpd(danishuni[,2], u, tau, indice_cola)
```

Como podemos observar, la distribucion generalizada de pareto no se ajusta correctamente a los valores mas extremos. Sin embargo, a los valores ubicados del 99% lo realiza correctamente por lo que no es tan mal modelo.

```{r PP Plot DGP}
#P-P Plot para la Dist. Generalizada de Pareto (DGP)

ppgpd <- function(data, u, tau, xi){
  excess <- data[data > u]
  m <- length(excess)
  emp.prob <- 1:m / (m + 1)
  prob.hat <- 1 - (1 + xi * (sort(excess) - u) / tau)^(-1/xi)
  plot(emp.prob, prob.hat, xlab = "Probabilidades empiricas",
         ylab = "Probabilidades ajustadas", xlim = c(0, 1),
         ylim = c(0, 1))
  abline(0, 1, col = "grey")
}

ppgpd(danishuni[,2], u, tau, indice_cola) 
```
El grafico PP-Plot muestran que las probabilidades generadas por el GDP se ajustan adecuadamente a los datos reales.

### Comunicacion de Resultados: Distribucion de Perdidas Agregadas


```{r}
pburrsum <- function(x, dfreq, argfreq, shape1, shape2, scale, Nmax=10)
  {
    tol <- 1e-10; maxit <- 10
    nbclaim <- 0:Nmax
    dnbclaim <- do.call(dfreq, c(list(x=nbclaim), argfreq))
    psumfornbclaim <- sapply(nbclaim, function(n)
      pburr(x, shape1=shape1, shape2 = shape2, scale = scale))
    psumtot <- psumfornbclaim %*% dnbclaim
    dnbclaimtot <- dnbclaim
    iter <- 0
    while( abs(sum(dnbclaimtot)-1) > tol && iter < maxit)
      {
        nbclaim <- nbclaim+Nmax
        dnbclaim <- do.call(dfreq, c(list(x=nbclaim), argfreq))
        psumfornbclaim <- sapply(nbclaim, function(n)
          pburr(x, shape1=shape1, shape2 = shape2, scale = scale))
        psumtot <- psumtot + psumfornbclaim %*% dnbclaim
        dnbclaimtot <- c(dnbclaimtot, dnbclaim)
        iter <- iter+1
         }
    as.numeric(psumtot)
}
```


```{r Distribucion de perdidas agregadas}
  # Parametros de la distribucion Burr
shape1 <- fit_burr_MLE$estimate[1]
shape2 <- fit_burr_MLE$estimate[2]
scale <- fit_burr_MLE$estimate[3]

  # Parametros de la distribucion Poisson

lambda <- fit_poisson_MLE$estimate[1]

  # N sigue una Poisson(0.5395916) y X- sigue una Burr(shape1, shape2, scale)

  # Severidad

  # Momento de orden 1 Burr
meansev <- mburr(1, shape1 = shape1, shape2 = shape2, scale = scale) 
  
  #Momento de orden 2 burr
varsev <- mburr(2, shape1 = shape1, shape2 = shape2, scale = scale) - meansev^2 

  #Coef.Asimetria Burr
skewsev <- (mburr(3, shape1 = shape1, shape2 = shape2, scale = scale) - 3*meansev*varsev - meansev^3)/varsev^(3/2) 

  # Frecuencia

#Hacemos lo mismo para la frecuencia.

lambda <- 0.5395916
meanfreq <- varfreq <- lambda; 
skewfreq <- 1/sqrt(lambda) #Momento de orden 1 Poisson

  # V. Agregada

  #Decimos que la media de la dist agregada es el producto de la frecuenca por la severidad.
meanagg <- meanfreq * meansev # Momento 1 Variable agregeda

# hacemos lo mismo con la varianza
varagg <- varfreq * (varsev + meansev^2) # Varianza v. agregada

# Coef.asimetria agre
skewagg <- (skewfreq*varfreq^(3/2)*meansev^3 + 3*varfreq*meansev*varsev + meanfreq*skewsev*varsev^(3/2))/varagg^(3/2) 

```

```{r Distribucion de perdidas agregadas}
  # 1. Simulacion

#agregatedist lo que hace es agregar la distribucion de unos modelos en concreto. De la Burr y de la poisson.
F.s <- aggregateDist("simulation", model.freq = expression(y =rpois(lambda)),
                     model.sev = expression(y = rburr(shape1 = 0.100000, shape2 = 14.441676, scale = 1.085243)),
                     nb.simul = 1000) 

  # 2. Aproximacion a traves de Normal

F.n <- aggregateDist("normal", moments = c(meanagg, varagg)) #los momentos introducimos los dos parametros que tiene la normal.
# mediaagregada y varianzaagregada de la normal


  # 3. Aproximacion a traves de normal-power

F.np <- aggregateDist("npower", moments = c(meanagg, varagg, skewagg))

```

```{r Plot de distr. perdidas agregadas por simulacion}
F.exact <- function(x) pburrsum(x, dpois, list(lambda=lambda),
                                shape1, shape2, scale, Nmax=100)

x <- seq(0,40) #Cambiar a 0,40

plot(x, F.exact(x), type="l",
        main="Distribución Agregada de pérdidas", ylab="F(x)")
lines(x, F.s(x), lty=2, col = "red")
lines(x, F.n(x), lty=3,col = "blue" )
lines(x, F.np(x), lty=4, col = "green")
legend("bottomright", leg=c("exacta", "simulacion",
                              "Aprox.normal", "Approx.NP"),
       col = c("black", "red", "blue", "green"),
       lty = 1:4, text.col = "black")
```

## Apendice VaR y CVaR

Se trata de un método para cuantificar la exposición al riesgo de mercado, utilizando técnicas estadísticas tradicionales. Partamos de la base de que los agentes económicos de hoy enfrentan riesgos de diferente naturaleza, como por ejemplo de crédito, de mercado, de liquidez, operacional, legal, etc. El Valor en Riesgo vendría a medir la pérdida que se podría sufrir en condiciones normales de mercado en un intervalo de tiempo y con un cierto nivel de probabilidad o de confianza.

El CVaR es el resultado de tomar el promedio ponderado de las observaciones de las cuales la pérdida excede el VaR. Por lo tanto, el CVaR supera la estimación del VaR, ya que puede cuantificar situaciones más arriesgadas, complementando así la información que brinda el VaR.

```{r}
## Cálculo del VAR

# Empleamos una generalizada de Pareto
danish <- data_sev$loss
cuantil <- 0.99
u <- quantile(danish, cuantil,names = FALSE)
fit.danish <- fit.GPD(danish, threshold = u)
(xi.hat.danish <- fit.danish$par.ses[["xi"]])
(beta.hat.danish <- fit.danish$par.ses[["beta"]])

#Calculamos  el exceso:

loss.excess <- danish[danish > u] - u

n.relative.excess <- length(loss.excess)/length(danish)

#Se calcula el VaR

(VaR.gpd <- u + (beta.hat.danish/xi.hat.danish) * 
    (((1 - cuantil)/n.relative.excess)^(-xi.hat.danish) - 
       1))

# Se calcula el VaR condicional:

(ES.gpd <- (VaR.gpd + beta.hat.danish - 
              xi.hat.danish * u)/(1 - xi.hat.danish))
```

Se calcula el VaR y el CVaR al 99% reportando unas perdidas de 26.11 y 34 millones de Coronas Danesas respectivamente. 

Ahora simulamos un caso en el cual el rendimiento anual de las acciones se distribuye con una media de 0.025 y una desviacion estander de 0.10. Un inversor decide comprar EUR 50000: 

A) Determinar el VaR 0.95 y 0.99 a un año.
B) Determinar el CVaR 0.95 y 0.99 a un año.
C) Representacion grafica.


```{r}
# NOTA PREVIA

alpha <- c(0.025)
q=qnorm(alpha, mean=0.025, sd=0.10) 
q


R=q*50000 #Perdida con rentabilidad negativa
R

Var=-R  #VaR,cambia el signo a R, para que resulte valor positivo
Var
#######


## CASO 1

#  VaR
alpha <- c(0.95, 0.99)
qnorm(alpha, mean=-0.025, sd=0.10) * 50000


# Tambien cambiando de signo a la media (De la tipificacion)

(-0.025 + 0.10 * qnorm(alpha)) * 50000


# ES

(-0.025 + 0.10 * dnorm(qnorm(alpha))/(1 - alpha)) * 50000


# Representamos ambos casos

x <- seq(0.9,0.999, length=100)

yVaR <- (-0.025 + 0.10 * qnorm(x)) * 50000
yES <- (-0.025 + 0.10 * dnorm(qnorm(x))/(1 - x)) * 50000

plot(x, yVaR, type="l", ylim=range(yVaR, yES),
     xlab=expression(alpha), ylab="")

lines(x, yES, lty=2, col=2)
legend("topleft", legend=c("VaR","ES"),col=1:2, lty=1:2)

```

